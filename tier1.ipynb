{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32764c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional: Install optuna for hyperparameter tuning\n",
    "# !pip install optuna -q\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"The official SMAPE metric function.\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / (denominator + 1e-8)) * 100\n",
    "\n",
    "def smape_objective(y_true_log, y_pred_log):\n",
    "    \"\"\"Custom objective function for LightGBM to learn SMAPE.\"\"\"\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    grad_numerator = 2 * (y_pred - y_true) * (np.abs(y_true) + np.abs(y_pred)) - 2 * np.abs(y_pred - y_true) * np.sign(y_pred)\n",
    "    grad_denominator = (np.abs(y_true) + np.abs(y_pred))**2\n",
    "    grad = grad_numerator / (grad_denominator + 1e-8)\n",
    "    grad = grad * y_pred\n",
    "    hess = np.ones_like(y_true)\n",
    "    return grad, hess\n",
    "\n",
    "def smape_lgbm_eval(y_true_log, y_pred_log):\n",
    "    \"\"\"Custom evaluation metric for LightGBM to show SMAPE score during training.\"\"\"\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    score = smape(y_true, y_pred)\n",
    "    return 'smape_eval', score, False # The False means a lower score is better\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "BASE_PATH = \"/content/drive/MyDrive/images\"\n",
    "TRAIN_CSV_PATH = f\"{BASE_PATH}/local_datasets/train_local.csv\"\n",
    "TEST_CSV_PATH = f\"{BASE_PATH}/local_datasets/test_local.csv\"\n",
    "TEXT_EMB_PATH = f\"{BASE_PATH}/text_embeddings/train_text_embeddings.npy\"\n",
    "IMG_EMB_PATH = f\"{BASE_PATH}/image_embeddings/train_image_embeddings.npy\"\n",
    "TEST_IMG_EMB_PATH = f\"{BASE_PATH}/image_embeddings/test_image_embeddings.npy\"\n",
    "OUTPUT_PATH = \"/content/drive/MyDrive/images/predictions/test_out_TIER1_SMAPE_FIX.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "print(\"Loading training data...\")\n",
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# Load embeddings\n",
    "print(\"Loading embeddings...\")\n",
    "text_embeddings = np.load(TEXT_EMB_PATH)\n",
    "image_embeddings = np.load(IMG_EMB_PATH)\n",
    "test_image_embeddings = np.load(TEST_IMG_EMB_PATH)\n",
    "\n",
    "# Generate test text embeddings if needed\n",
    "\n",
    "test_text_embeddings = np.load(f\"{BASE_PATH}/text_embeddings/test_text_embeddings.npy\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: ADVANCED FEATURE ENGINEERING (ENHANCED)\n",
    "# ============================================================================\n",
    "print(\"\\nEngineering advanced features (Enhanced)...\")\n",
    "\n",
    "def extract_catalog_features(catalog_text):\n",
    "    \"\"\"Extract structured features from catalog content - FULLY ENHANCED version\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    if not isinstance(catalog_text, str):\n",
    "        catalog_text = \"\"\n",
    "\n",
    "    catalog_text = catalog_text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "    catalog_text_lower = catalog_text.lower()\n",
    "\n",
    "    # --- Basic Features (Extended) ---\n",
    "    features['catalog_length'] = len(catalog_text)\n",
    "    features['word_count'] = len(catalog_text.split())\n",
    "    features['has_quantity'] = int('Value:' in catalog_text)\n",
    "    features['has_unit'] = int('Unit:' in catalog_text)\n",
    "    features['has_description'] = int('Product Description:' in catalog_text)\n",
    "    features['uppercase_ratio'] = sum(1 for c in catalog_text if c.isupper()) / max(len(catalog_text), 1)\n",
    "    features['digit_count'] = sum(1 for c in catalog_text if c.isdigit())\n",
    "    features['special_char_count'] = sum(1 for c in catalog_text if not c.isalnum() and not c.isspace())\n",
    "\n",
    "    # --- Quantity Value ---\n",
    "    quantity_match = re.search(r\"Value:\\s*([\\d\\.]+)\", catalog_text)\n",
    "    try:\n",
    "        features['quantity_value'] = float(quantity_match.group(1)) if quantity_match else 0\n",
    "    except:\n",
    "        features['quantity_value'] = 0\n",
    "\n",
    "    # --- Pack Size Extraction ---\n",
    "    pack_size = 1\n",
    "    pack_match = re.search(r'(?:pack of|case of|pk of)\\s*\\(?(\\d{1,3})\\)?|(\\d{1,3})\\s*(?:-pack|-count|ct|units)', catalog_text_lower)\n",
    "    if pack_match:\n",
    "        size_str = next((s for s in pack_match.groups() if s is not None), None)\n",
    "        if size_str:\n",
    "            try:\n",
    "                pack_size = int(size_str)\n",
    "            except:\n",
    "                pack_size = 1\n",
    "    features['pack_size'] = pack_size\n",
    "    \n",
    "    # --- Total Quantity (NEW) - Interaction between quantity and pack size ---\n",
    "    features['total_quantity'] = features['quantity_value'] * pack_size\n",
    "\n",
    "    # --- Unit Standardization ---\n",
    "    unit_match = re.search(r\"Unit:\\s*(\\w+)\", catalog_text)\n",
    "    unit_raw = unit_match.group(1).strip().lower() if unit_match else 'none'\n",
    "\n",
    "    if re.match(r'o(u)?nce|oz|fl oz', unit_raw):\n",
    "        features['unit_std'] = 'Ounce'\n",
    "    elif re.match(r'count|ct|each|piece|units|tea', unit_raw):\n",
    "        features['unit_std'] = 'Count'\n",
    "    elif re.match(r'pound|lb', unit_raw):\n",
    "        features['unit_std'] = 'Pound'\n",
    "    elif re.match(r'fl|fluid|ml|millilitre|liters|ltr|volume|gallon', unit_raw):\n",
    "        features['unit_std'] = 'Volume'\n",
    "    elif re.match(r'gram|gr|kg', unit_raw):\n",
    "        features['unit_std'] = 'Gram'\n",
    "    elif re.match(r'case|pack|box|bag|jar|bottle|pouch|bucket|can', unit_raw):\n",
    "        features['unit_std'] = 'Container'\n",
    "    else:\n",
    "        features['unit_std'] = 'Other'\n",
    "\n",
    "    # --- Bullet Point Count and Description Length ---\n",
    "    features['bullet_point_count'] = len(re.findall(r\"Bullet Point \\d+:\", catalog_text))\n",
    "    \n",
    "    desc_match = re.search(r\"Product Description:\\s*(.*)\", catalog_text, re.DOTALL)\n",
    "    if desc_match:\n",
    "        desc_text = desc_match.group(1).strip()\n",
    "        features['description_length'] = len(desc_text)\n",
    "        features['description_word_count'] = len(desc_text.split())\n",
    "    else:\n",
    "        features['description_length'] = 0\n",
    "        features['description_word_count'] = 0\n",
    "\n",
    "    # --- Item Name / Brand Proxy Extraction ---\n",
    "    item_name_match = re.search(r\"Item Name:\\s*(.*?)(?:Bullet Point 1:|Product Description:|Value:|$)\", catalog_text, re.DOTALL)\n",
    "    item_name = item_name_match.group(1).strip() if item_name_match else ''\n",
    "    \n",
    "    words = item_name.split()\n",
    "    brand_proxy = words[0].lower() if words else 'none'\n",
    "    \n",
    "    if brand_proxy in ['the', 'a', 'deluxe', 'premium', 'original', 'gourmet', 'best', 'traditional', 'goya']:\n",
    "        brand_proxy = \" \".join(words[:2]).lower() if len(words) >= 2 else 'none'\n",
    "\n",
    "    features['brand_proxy'] = brand_proxy\n",
    "    \n",
    "    # --- Quality & Health Keywords ---\n",
    "    premium_words = ['premium', 'gourmet', 'deluxe', 'artisanal', 'handcrafted', 'superior', 'luxury']\n",
    "    health_words = ['organic', 'natural', 'healthy', 'non-gmo', 'usda organic']\n",
    "    dietary_words = ['gluten-free', 'gluten free', 'sugar-free', 'vegan', 'keto', 'paleo']\n",
    "    \n",
    "    features['is_premium'] = int(any(word in catalog_text_lower for word in premium_words))\n",
    "    features['is_health_focused'] = int(any(word in catalog_text_lower for word in health_words))\n",
    "    features['has_dietary_claim'] = int(any(word in catalog_text_lower for word in dietary_words))\n",
    "\n",
    "    # --- Product Form ---\n",
    "    features['form_powder'] = int('powder' in catalog_text_lower or 'mix' in catalog_text_lower)\n",
    "    features['form_liquid'] = int('liquid' in catalog_text_lower or 'juice' in catalog_text_lower or \n",
    "                                   'sauce' in catalog_text_lower or 'drink' in catalog_text_lower)\n",
    "    features['form_solid'] = int('bar' in catalog_text_lower or 'solid' in catalog_text_lower or \n",
    "                                  'cookies' in catalog_text_lower or 'crackers' in catalog_text_lower)\n",
    "    features['form_capsule'] = int('capsule' in catalog_text_lower or 'pod' in catalog_text_lower or \n",
    "                                    'k-cup' in catalog_text_lower or 'k cup' in catalog_text_lower)\n",
    "\n",
    "    # --- Purpose & Audience ---\n",
    "    gift_words = ['gift', 'basket', 'assortment', 'variety pack', 'sampler', 'housewarming']\n",
    "    features['is_for_gifting'] = int(any(word in catalog_text_lower for word in gift_words))\n",
    "    features['is_bulk'] = int('bulk' in catalog_text_lower or 'food service' in catalog_text_lower or \n",
    "                              'industrial' in catalog_text_lower)\n",
    "\n",
    "    # --- Origin & Packaging ---\n",
    "    features['made_in_usa'] = int('made in usa' in catalog_text_lower or 'made in the usa' in catalog_text_lower)\n",
    "    features['is_imported'] = int('imported' in catalog_text_lower or 'product of' in catalog_text_lower)\n",
    "    features['is_resealable'] = int('resealable' in catalog_text_lower or 'reseal' in catalog_text_lower)\n",
    "    \n",
    "    # Packaging type detection\n",
    "    packaging_keywords = ['bag', 'box', 'can', 'canister', 'jar', 'bottle', 'bowl', 'pouch', 'tube', 'case']\n",
    "    found_packaging = 'other'\n",
    "    for pkg in packaging_keywords:\n",
    "        if pkg in catalog_text_lower:\n",
    "            found_packaging = pkg\n",
    "            break\n",
    "    features['packaging_type'] = found_packaging\n",
    "\n",
    "    # --- Additional Keywords ---\n",
    "    features['has_pack'] = int('pack' in catalog_text_lower or 'case' in catalog_text_lower)\n",
    "    features['has_bundle'] = int('bundle' in catalog_text_lower or 'assortment' in catalog_text_lower)\n",
    "    features['has_discount'] = int('discount' in catalog_text_lower or 'sale' in catalog_text_lower)\n",
    "    features['has_professional'] = int('professional' in catalog_text_lower or 'bulk' in catalog_text_lower)\n",
    "    features['has_gluten_free'] = int('gluten-free' in catalog_text_lower or 'gluten free' in catalog_text_lower)\n",
    "    features['has_kosher'] = int('kosher' in catalog_text_lower or 'dairy certified' in catalog_text_lower)\n",
    "    features['has_organic'] = int('organic' in catalog_text_lower)\n",
    "    features['has_sugar_free'] = int('sugar-free' in catalog_text_lower or 'no sugar added' in catalog_text_lower)\n",
    "    features['has_natural'] = int('natural' in catalog_text_lower or 'all natural' in catalog_text_lower)\n",
    "    features['has_vegan'] = int('vegan' in catalog_text_lower or 'plant-based' in catalog_text_lower)\n",
    "    features['has_certified'] = int('certified' in catalog_text_lower or 'gmp' in catalog_text_lower or \n",
    "                                     'fda' in catalog_text_lower or 'fair trade' in catalog_text_lower)\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_catalog_features_df(df):\n",
    "    \"\"\"Applies the extraction to the dataframe and returns a new feature dataframe.\"\"\"\n",
    "    print(\"Applying catalog feature extraction...\")\n",
    "    catalog_features_list = [extract_catalog_features(text) for text in df['catalog_content']]\n",
    "    features_df = pd.DataFrame(catalog_features_list)\n",
    "    features_df['sample_id'] = df['sample_id'].values\n",
    "    return features_df\n",
    "\n",
    "# --- 2a. Extract base catalog features ---\n",
    "cat_train_df = extract_catalog_features_df(df_train)\n",
    "cat_test_df = extract_catalog_features_df(df_test)\n",
    "\n",
    "# --- 2b. Categorical Feature Processing (Unit/Brand/Packaging OHE) ---\n",
    "\n",
    "N_BRAND_CATEGORIES = 20\n",
    "brand_counts = cat_train_df['brand_proxy'].value_counts()\n",
    "top_brands = brand_counts[brand_counts > 5].index.tolist()\n",
    "top_brands = top_brands[:N_BRAND_CATEGORIES]\n",
    "print(f\"Using {len(top_brands)} top brands for OHE.\")\n",
    "\n",
    "def process_categorical_features(df, top_brands):\n",
    "    \"\"\"Process categorical features with OHE for unit, brand, AND packaging\"\"\"\n",
    "    df_cat = df[['unit_std', 'brand_proxy', 'packaging_type']].copy()\n",
    "    \n",
    "    # 1. Unit OHE (Use all standardized units)\n",
    "    unit_dummies = pd.get_dummies(df_cat['unit_std'], prefix='unit', dummy_na=False)\n",
    "    df_cat = pd.concat([df_cat, unit_dummies], axis=1)\n",
    "    \n",
    "    # 2. Brand OHE (Use only the top brands)\n",
    "    df_cat['brand_proxy_other'] = df_cat['brand_proxy'].apply(lambda x: x if x in top_brands else 'Other')\n",
    "    brand_dummies = pd.get_dummies(df_cat['brand_proxy_other'], prefix='brand', dummy_na=False)\n",
    "    df_cat = pd.concat([df_cat, brand_dummies], axis=1)\n",
    "    \n",
    "    # 3. Packaging OHE (NEW) - One-hot encode all packaging types\n",
    "    packaging_dummies = pd.get_dummies(df_cat['packaging_type'], prefix='pkg', dummy_na=False)\n",
    "    df_cat = pd.concat([df_cat, packaging_dummies], axis=1)\n",
    "    \n",
    "    # Remove original columns\n",
    "    df_cat = df_cat.drop(columns=['unit_std', 'brand_proxy', 'brand_proxy_other', 'packaging_type'], errors='ignore')\n",
    "    return df_cat\n",
    "\n",
    "cat_train_ohe = process_categorical_features(cat_train_df, top_brands)\n",
    "cat_test_ohe = process_categorical_features(cat_test_df, top_brands)\n",
    "\n",
    "# Align test columns with train columns (CRITICAL for OHE consistency)\n",
    "missing_cols = set(cat_train_ohe.columns) - set(cat_test_ohe.columns)\n",
    "for c in missing_cols:\n",
    "    cat_test_ohe[c] = 0\n",
    "cat_test_ohe = cat_test_ohe[cat_train_ohe.columns]\n",
    "print(f\"Train OHE shape: {cat_train_ohe.shape}, Test OHE shape: {cat_test_ohe.shape}\")\n",
    "\n",
    "# Drop un-encoded categoricals from the main catalog feature DF\n",
    "cat_train_df = cat_train_df.drop(columns=['unit_std', 'brand_proxy', 'packaging_type'])\n",
    "cat_test_df = cat_test_df.drop(columns=['unit_std', 'brand_proxy', 'packaging_type'])\n",
    "\n",
    "# Merge OHE back into the main feature dataframes\n",
    "cat_train_final = pd.concat([cat_train_df, cat_train_ohe], axis=1)\n",
    "cat_test_final = pd.concat([cat_test_df, cat_test_ohe], axis=1)\n",
    "\n",
    "\n",
    "def create_feature_matrix(df, text_emb, img_emb, catalog_features_df):\n",
    "    \"\"\"Create comprehensive feature matrix\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    features['sample_id'] = df['sample_id'].values\n",
    "    \n",
    "    # ===== Embedding features =====\n",
    "    # Raw embedding dimensions\n",
    "    for i in range(text_emb.shape[1]):\n",
    "        features[f'text_emb_{i}'] = text_emb[:, i]\n",
    "    \n",
    "    for i in range(img_emb.shape[1]):\n",
    "        features[f'img_emb_{i}'] = img_emb[:, i]\n",
    "    \n",
    "    # Statistical features from embeddings\n",
    "    features['text_emb_mean'] = text_emb.mean(axis=1)\n",
    "    features['text_emb_std'] = text_emb.std(axis=1)\n",
    "    features['text_emb_max'] = text_emb.max(axis=1)\n",
    "    features['text_emb_min'] = text_emb.min(axis=1)\n",
    "    features['text_emb_range'] = features['text_emb_max'] - features['text_emb_min']\n",
    "    features['text_emb_median'] = np.median(text_emb, axis=1)\n",
    "    features['text_emb_skew'] = pd.DataFrame(text_emb).skew(axis=1).values\n",
    "    features['text_emb_kurtosis'] = pd.DataFrame(text_emb).kurtosis(axis=1).values\n",
    "    \n",
    "    features['img_emb_mean'] = img_emb.mean(axis=1)\n",
    "    features['img_emb_std'] = img_emb.std(axis=1)\n",
    "    features['img_emb_max'] = img_emb.max(axis=1)\n",
    "    features['img_emb_min'] = img_emb.min(axis=1)\n",
    "    features['img_emb_range'] = features['img_emb_max'] - features['img_emb_min']\n",
    "    features['img_emb_median'] = np.median(img_emb, axis=1)\n",
    "    \n",
    "    # L2 norms\n",
    "    features['text_emb_l2'] = np.sqrt((text_emb ** 2).sum(axis=1))\n",
    "    features['img_emb_l2'] = np.sqrt((img_emb ** 2).sum(axis=1))\n",
    "    \n",
    "    # Interaction features\n",
    "    # features['emb_cosine_sim'] = (text_emb * img_emb).sum(axis=1) / (features['text_emb_l2'] * features['img_emb_l2'] + 1e-8)\n",
    "    features['emb_interaction_mean'] = features['text_emb_mean'] * features['img_emb_mean']\n",
    "    features['emb_interaction_std'] = features['text_emb_std'] * features['img_emb_std']\n",
    "    features['emb_l2_dist'] = features['text_emb_l2'] + features['img_emb_l2']\n",
    "    features['emb_mean_ratio'] = features['text_emb_mean'] / (features['img_emb_mean'] + 1e-8)\n",
    "    \n",
    "    # ===== Catalog content features =====\n",
    "    catalog_cols = [col for col in catalog_features_df.columns if col != 'sample_id']\n",
    "    for col in catalog_cols:\n",
    "        features[f'cat_{col}'] = catalog_features_df[col].values\n",
    "        \n",
    "    # ===== Combined features (REVISED) =====\n",
    "    features['emb_text_to_catalog_ratio'] = features['text_emb_l2'] / (features['cat_catalog_length'] + 1e-8)\n",
    "    features['emb_img_to_quantity_ratio'] = features['img_emb_l2'] / (features['cat_quantity_value'] + 1e-8)\n",
    "    features['cat_description_to_catalog_ratio'] = features['cat_description_length'] / (features['cat_catalog_length'] + 1e-8)\n",
    "    features['cat_words_per_bp'] = features['cat_word_count'] / (features['cat_bullet_point_count'] + 1e-8)\n",
    "    \n",
    "    # ===== NEW: Price-per-unit proxy and total quantity interaction =====\n",
    "    features['price_unit_proxy'] = features['img_emb_l2'] / (features['cat_total_quantity'] + 1e-8)\n",
    "    features['quantity_complexity'] = features['cat_total_quantity'] * features['cat_bullet_point_count']\n",
    "    \n",
    "    return features\n",
    "\n",
    "X_train = create_feature_matrix(df_train, text_embeddings, image_embeddings, cat_train_final)\n",
    "X_test = create_feature_matrix(df_test, test_text_embeddings, test_image_embeddings, cat_test_final)\n",
    "\n",
    "y_train = df_train['price'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Test feature matrix shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: DATA PREPROCESSING & OUTLIER HANDLING\n",
    "# ============================================================================\n",
    "print(\"\\nPreprocessing data...\")\n",
    "\n",
    "price_mean = y_train.mean()\n",
    "price_std = y_train.std()\n",
    "mask = (y_train >= price_mean - 3.5 * price_std) & (y_train <= price_mean + 3.5 * price_std)\n",
    "\n",
    "X_train_clean = X_train[mask].reset_index(drop=True)\n",
    "y_train_clean = y_train[mask]\n",
    "\n",
    "print(f\"Removed {len(y_train) - len(y_train_clean)} outliers\")\n",
    "print(f\"Price distribution: min={y_train_clean.min():.2f}, max={y_train_clean.max():.2f}, mean={y_train_clean.mean():.2f}\")\n",
    "\n",
    "# Handle missing values\n",
    "X_train_clean = X_train_clean.fillna(X_train_clean.median())\n",
    "X_test = X_test.fillna(X_train_clean.median())\n",
    "\n",
    "# Feature scaling\n",
    "scaler = RobustScaler()\n",
    "feature_cols = [col for col in X_train_clean.columns if col != 'sample_id']\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean[feature_cols])\n",
    "X_test_scaled = scaler.transform(X_test[feature_cols])\n",
    "\n",
    "# Log transformation\n",
    "print(\"\\nApplying log transformation...\")\n",
    "y_train_log = np.log1p(y_train_clean)\n",
    "print(f\"Original price - mean: {y_train_clean.mean():.2f}, std: {y_train_clean.std():.2f}\")\n",
    "print(f\"Log price - mean: {y_train_log.mean():.2f}, std: {y_train_log.std():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: HYPERPARAMETER TUNING WITH OPTUNA\n",
    "# ============================================================================\n",
    "print(\"\\nTuning hyperparameters with Optuna...\")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n",
    "    # We only tune LGBM for this focused run\n",
    "    params = {\n",
    "        'lgb_lr': trial.suggest_float('lgb_lr', 0.01, 0.1),\n",
    "        'lgb_depth': trial.suggest_int('lgb_depth', 5, 10),\n",
    "        # Add other lgbm params to tune if desired\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "    }\n",
    "    \n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train_scaled):\n",
    "        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "        \n",
    "        # --- FIX #4: Use the upgraded LightGBM model training ---\n",
    "        lgb_model = LGBMRegressor(\n",
    "            objective='regression_l1',\n",
    "            n_estimators=2000,\n",
    "            learning_rate=params['lgb_lr'],\n",
    "            max_depth=params['lgb_depth'],\n",
    "            num_leaves=params['num_leaves'],\n",
    "            feature_fraction=params['feature_fraction'],\n",
    "            bagging_fraction=params['bagging_fraction'],\n",
    "            bagging_freq=1,\n",
    "            random_state=42, \n",
    "            n_jobs=-1, \n",
    "            verbose=-1\n",
    "        )\n",
    "        lgb_model.fit(X_tr, y_tr,\n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      eval_metric=smape_lgbm_eval,\n",
    "                      callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        \n",
    "        lgb_pred = lgb_model.predict(X_val)\n",
    "        \n",
    "        pred_price = np.expm1(lgb_pred)\n",
    "        y_val_original = np.expm1(y_val)\n",
    "        \n",
    "        smape_score = smape(y_val_original, pred_price)\n",
    "        cv_scores.append(smape_score)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "try:\n",
    "    study = optuna.create_study(direction='minimize', pruner=MedianPruner(), study_name=\"price_tuning\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=True, catch=(Exception,))\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest parameters found:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best CV SMAPE: {study.best_value:.4f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOptuna error: {e}. Using default parameters.\")\n",
    "    best_params = {\n",
    "        'xgb_lr': 0.05, 'xgb_depth': 7,\n",
    "        'lgb_lr': 0.05, 'lgb_depth': 7,\n",
    "        'cat_lr': 0.05, 'cat_depth': 6,\n",
    "        'gb_lr': 0.05, 'gb_depth': 6,\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TRAIN FINAL MODELS WITH TUNED HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "print(\"\\nTraining final models with tuned hyperparameters...\")\n",
    "\n",
    "models = {\n",
    "    'xgb': XGBRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=int(best_params.get('xgb_depth', 7)),\n",
    "        learning_rate=best_params.get('xgb_lr', 0.05),\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    'lgb': LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=int(best_params.get('lgb_depth', 7)),\n",
    "        learning_rate=best_params.get('lgb_lr', 0.05),\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'catboost': CatBoostRegressor(\n",
    "        iterations=400,\n",
    "        depth=int(best_params.get('cat_depth', 6)),\n",
    "        learning_rate=best_params.get('cat_lr', 0.05),\n",
    "        subsample=0.85,\n",
    "        verbose=0,\n",
    "        random_state=42,\n",
    "        thread_count=-1\n",
    "    ),\n",
    "    'gb': GradientBoostingRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=int(best_params.get('gb_depth', 6)),\n",
    "        learning_rate=best_params.get('gb_lr', 0.05),\n",
    "        subsample=0.85,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'histgb': HistGradientBoostingRegressor(\n",
    "        max_iter=400,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        loss='squared_error'\n",
    "    ),\n",
    "    'ridge': Ridge(alpha=0.5),\n",
    "    'lasso': Lasso(alpha=0.05),\n",
    "    'elasticnet': ElasticNet(alpha=0.05, l1_ratio=0.5),\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "train_preds = {}\n",
    "test_preds = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_scaled, y_train_log)\n",
    "    train_preds[name] = model.predict(X_train_scaled)\n",
    "    test_preds[name] = model.predict(X_test_scaled)\n",
    "    gc.collect()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: ADVANCED STACKING WITH MULTIPLE META-LEARNERS\n",
    "# ============================================================================\n",
    "print(\"\\nTraining meta-learners...\")\n",
    "\n",
    "X_meta_train = np.column_stack([train_preds[name] for name in models.keys()])\n",
    "X_meta_test = np.column_stack([test_preds[name] for name in models.keys()])\n",
    "\n",
    "meta_models = {\n",
    "    'meta_ridge': Ridge(alpha=0.1),\n",
    "    'meta_ridge_strong': Ridge(alpha=1.0),\n",
    "    'meta_lasso': Lasso(alpha=0.01),\n",
    "}\n",
    "\n",
    "meta_preds_train = {}\n",
    "meta_preds_test = {}\n",
    "\n",
    "for meta_name, meta_model in meta_models.items():\n",
    "    meta_model.fit(X_meta_train, y_train_log)\n",
    "    meta_preds_train[meta_name] = meta_model.predict(X_meta_train)\n",
    "    meta_preds_test[meta_name] = meta_model.predict(X_meta_test)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: FINAL ENSEMBLE WITH WEIGHTED AVERAGING\n",
    "# ============================================================================\n",
    "print(\"Creating final ensemble...\")\n",
    "\n",
    "# Get weights from first meta-learner\n",
    "meta_model_final = meta_models['meta_ridge']\n",
    "weights = np.abs(meta_model_final.coef_) / np.abs(meta_model_final.coef_).sum()\n",
    "print(f\"Model weights: {dict(zip(models.keys(), np.round(weights, 3)))}\")\n",
    "\n",
    "# Weighted ensemble\n",
    "ensemble_train_log = np.average(X_meta_train, axis=1, weights=weights)\n",
    "ensemble_test_log = np.average(X_meta_test, axis=1, weights=weights)\n",
    "\n",
    "# Convert back from log scale\n",
    "ensemble_train = np.expm1(ensemble_train_log)\n",
    "ensemble_test = np.expm1(ensemble_test_log)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: EVALUATE ON TRAINING SET\n",
    "# ============================================================================\n",
    "def smape(actual, predicted):\n",
    "    return 100 * np.mean(2 * np.abs(predicted - actual) / (np.abs(actual) + np.abs(predicted)))\n",
    "\n",
    "train_smape = smape(y_train_clean, ensemble_train)\n",
    "print(f\"\\nTraining SMAPE: {train_smape:.4f}%\")\n",
    "\n",
    "# Cross-validation evaluation\n",
    "print(\"Running 5-fold cross-validation...\")\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_scaled)):\n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    fold_preds = []\n",
    "    for name, model in models.items():\n",
    "        # Re-initialize model to avoid training on previous fold's data\n",
    "        m = model.__class__(**{k: v for k, v in model.get_params().items() if not k.startswith('_')})\n",
    "        m.fit(X_tr, y_tr)\n",
    "        fold_preds.append(m.predict(X_val))\n",
    "    \n",
    "    X_meta_val = np.column_stack(fold_preds)\n",
    "    fold_ensemble_log = np.average(X_meta_val, axis=1, weights=weights)\n",
    "    fold_ensemble = np.expm1(fold_ensemble_log)\n",
    "    y_val_original = np.expm1(y_val)\n",
    "    \n",
    "    fold_smape = smape(y_val_original, fold_ensemble)\n",
    "    cv_scores.append(fold_smape)\n",
    "    print(f\"  Fold {fold + 1}: {fold_smape:.4f}%\")\n",
    "\n",
    "print(f\"Cross-validation SMAPE: {np.mean(cv_scores):.4f}% ± {np.std(cv_scores):.4f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: PREPARE OUTPUT\n",
    "# ============================================================================\n",
    "print(\"\\nPreparing output...\")\n",
    "\n",
    "# Ensure predictions are positive\n",
    "ensemble_test = np.maximum(ensemble_test, 0.1)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'sample_id': df_test['sample_id'].values,\n",
    "    'price': ensemble_test\n",
    "})\n",
    "\n",
    "# Sort by sample_id\n",
    "output_df = output_df.sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {output_df.shape}\")\n",
    "print(f\"Price range: [{output_df['price'].min():.2f}, {output_df['price'].max():.2f}]\")\n",
    "print(f\"Mean price: {output_df['price'].mean():.2f}\")\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(output_df.head(10))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: SAVE PREDICTIONS\n",
    "# ============================================================================\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "output_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\n✅ Predictions saved to: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
