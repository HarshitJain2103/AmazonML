{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d23cb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "BASE_PATH = \"/content/drive/MyDrive/images\"\n",
    "TRAIN_CSV = f\"{BASE_PATH}/local_datasets/train_local.csv\"\n",
    "TEST_CSV = f\"{BASE_PATH}/local_datasets/test_local.csv\"\n",
    "\n",
    "# Use Nomic embeddings (768 dims)\n",
    "TRAIN_TEXT_EMB = f\"{BASE_PATH}/text_embeddings/train_text_embeddings_clip512.npy\"\n",
    "TEST_TEXT_EMB = f\"{BASE_PATH}/text_embeddings/test_text_embeddings_clip512.npy\"\n",
    "\n",
    "# CLIP image embeddings\n",
    "TRAIN_IMG_EMB = f\"{BASE_PATH}/image_embeddings/train_image_embeddings.npy\"\n",
    "TEST_IMG_EMB = f\"{BASE_PATH}/image_embeddings/test_image_embeddings.npy\"\n",
    "\n",
    "OUTPUT_CSV = f\"{BASE_PATH}/predictions/submission_final.csv\"\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 7\n",
    "USE_GPU = True\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Amazon ML Challenge - Advanced Pipeline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD DATA\n",
    "# =============================================================================\n",
    "print(\"\\n[1/12] Loading data...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "print(f\"Price stats - Min: ${train_df['price'].min():.2f}, Max: ${train_df['price'].max():.2f}, \"\n",
    "      f\"Median: ${train_df['price'].median():.2f}\")\n",
    "\n",
    "# Load embeddings\n",
    "X_text_train = np.load(TRAIN_TEXT_EMB).astype(np.float32)\n",
    "X_text_test = np.load(TEST_TEXT_EMB).astype(np.float32)\n",
    "X_img_train = np.load(TRAIN_IMG_EMB).astype(np.float32)\n",
    "X_img_test = np.load(TEST_IMG_EMB).astype(np.float32)\n",
    "\n",
    "print(f\"Text embeddings: {X_text_train.shape}\")\n",
    "print(f\"Image embeddings: {X_img_train.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ADVANCED CATALOG PARSING\n",
    "# =============================================================================\n",
    "print(\"\\n[2/12] Advanced catalog parsing...\")\n",
    "\n",
    "def extract_value_and_unit(text):\n",
    "    \"\"\"Extract Value and Unit from catalog\"\"\"\n",
    "    value_match = re.search(r'Value:\\s*([\\d\\.]+)\\s*\\n\\s*Unit:\\s*(\\w+)', str(text), re.IGNORECASE)\n",
    "    if value_match:\n",
    "        try:\n",
    "            return float(value_match.group(1)), value_match.group(2).lower()\n",
    "        except:\n",
    "            pass\n",
    "    return None, None\n",
    "\n",
    "def extract_quantity_info(text):\n",
    "    \"\"\"Extract quantity, pack size, weight info\"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    features = {}\n",
    "\n",
    "    # Pack detection: \"12 pack\", \"pack of 12\", \"12-pack\"\n",
    "    pack_patterns = [\n",
    "        r'(\\d+)[\\s-]*pack',\n",
    "        r'pack\\s+of\\s+(\\d+)',\n",
    "        r'(\\d+)[\\s-]*count',\n",
    "        r'(\\d+)\\s*ct\\b'\n",
    "    ]\n",
    "    pack_size = 1\n",
    "    for pattern in pack_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            pack_size = max(pack_size, int(match.group(1)))\n",
    "    features['pack_size'] = pack_size\n",
    "\n",
    "    # Multipack: \"6 x 500ml\", \"12 x 16oz\"\n",
    "    multipack = re.search(r'(\\d+)\\s*[x×]\\s*(\\d+\\.?\\d*)\\s*(ml|oz|g|l|kg)', text_lower)\n",
    "    if multipack:\n",
    "        features['is_multipack'] = 1\n",
    "        features['multipack_count'] = int(multipack.group(1))\n",
    "        features['unit_size'] = float(multipack.group(2))\n",
    "        features['unit_type'] = multipack.group(3)\n",
    "    else:\n",
    "        features['is_multipack'] = 0\n",
    "        features['multipack_count'] = 1\n",
    "\n",
    "        # Single unit extraction\n",
    "        unit_match = re.search(r'(\\d+\\.?\\d*)\\s*(oz|ounce|ml|liter|litre|gram|kg|lb|pound)s?\\b', text_lower)\n",
    "        if unit_match:\n",
    "            features['unit_size'] = float(unit_match.group(1))\n",
    "            features['unit_type'] = unit_match.group(2)[:2]  # Normalize\n",
    "        else:\n",
    "            features['unit_size'] = 0\n",
    "            features['unit_type'] = 'count'\n",
    "\n",
    "    # Total quantity estimate\n",
    "    unit_size = features.get('unit_size', 0)\n",
    "    multipack_count = features.get('multipack_count', 1)\n",
    "\n",
    "    # Convert everything to grams for normalization\n",
    "    total_grams = 0\n",
    "    if features.get('unit_type') in ['oz', 'ou']:\n",
    "        total_grams = unit_size * 28.35 * multipack_count * pack_size\n",
    "    elif features.get('unit_type') in ['ml', 'ml']:\n",
    "        total_grams = unit_size * multipack_count * pack_size  # Approximate ml as g\n",
    "    elif features.get('unit_type') in ['g', 'gr']:\n",
    "        total_grams = unit_size * multipack_count * pack_size\n",
    "    elif features.get('unit_type') in ['kg']:\n",
    "        total_grams = unit_size * 1000 * multipack_count * pack_size\n",
    "    elif features.get('unit_type') in ['l', 'li']:\n",
    "        total_grams = unit_size * 1000 * multipack_count * pack_size\n",
    "    elif features.get('unit_type') in ['lb', 'po']:\n",
    "        total_grams = unit_size * 453.592 * multipack_count * pack_size\n",
    "\n",
    "    features['total_grams_estimate'] = total_grams\n",
    "\n",
    "    return features\n",
    "\n",
    "def parse_catalog_comprehensive(text):\n",
    "    \"\"\"Comprehensive catalog parsing\"\"\"\n",
    "    text_str = str(text)\n",
    "    text_lower = text_str.lower()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Basic text stats\n",
    "    features['text_len'] = len(text_str)\n",
    "    features['word_count'] = len(text_str.split())\n",
    "    features['digit_count'] = sum(c.isdigit() for c in text_str)\n",
    "    features['upper_count'] = sum(c.isupper() for c in text_str)\n",
    "    features['special_count'] = sum(not c.isalnum() and not c.isspace() for c in text_str)\n",
    "\n",
    "    # Structural features\n",
    "    features['bullet_count'] = text_lower.count('bullet point')\n",
    "    features['has_description'] = int('product description:' in text_lower)\n",
    "    features['has_brand'] = int('brand:' in text_lower)\n",
    "    features['newline_count'] = text_str.count('\\n')\n",
    "    features['colon_count'] = text_str.count(':')\n",
    "\n",
    "    # Value and Unit extraction (CRITICAL!)\n",
    "    value, unit = extract_value_and_unit(text_str)\n",
    "    if value is not None:\n",
    "        features['value_extracted'] = value\n",
    "        features['has_value'] = 1\n",
    "        # Encode unit type\n",
    "        unit_map = {'kilogram': 1, 'pound': 2, 'ounce': 3, 'gram': 4,\n",
    "                   'ton': 5, 'count': 6, 'milliliter': 7, 'liter': 8}\n",
    "        features['unit_encoded'] = unit_map.get(unit, 0)\n",
    "    else:\n",
    "        features['value_extracted'] = 0\n",
    "        features['has_value'] = 0\n",
    "        features['unit_encoded'] = 0\n",
    "\n",
    "    # Quantity/pack info\n",
    "    qty_info = extract_quantity_info(text_str)\n",
    "    features.update(qty_info)\n",
    "\n",
    "    # Brand extraction (improved)\n",
    "    brand_match = re.search(r'Brand:\\s*([^\\n]+)', text_str, re.IGNORECASE)\n",
    "    if brand_match:\n",
    "        brand = brand_match.group(1).strip()[:30]\n",
    "    else:\n",
    "        # Try item name\n",
    "        name_match = re.search(r'Item Name:\\s*([^\\n]+)', text_str, re.IGNORECASE)\n",
    "        if name_match:\n",
    "            name = name_match.group(1).strip()\n",
    "            brand = name.split()[0] if name.split() else 'unknown'\n",
    "        else:\n",
    "            brand = 'unknown'\n",
    "\n",
    "    brand = re.sub(r'[^a-zA-Z0-9]', '', brand.lower())\n",
    "    features['brand'] = brand if brand else 'unknown'\n",
    "\n",
    "    # Category signals\n",
    "    categories = {\n",
    "        'food': ['food', 'snack', 'candy', 'chocolate', 'chip', 'cookie', 'beverage'],\n",
    "        'health': ['vitamin', 'supplement', 'protein', 'organic', 'health'],\n",
    "        'beauty': ['beauty', 'cosmetic', 'shampoo', 'soap', 'lotion', 'cream'],\n",
    "        'household': ['cleaner', 'detergent', 'paper towel', 'tissue', 'trash'],\n",
    "        'baby': ['baby', 'infant', 'diaper', 'wipe', 'formula'],\n",
    "        'pet': ['pet', 'dog', 'cat', 'animal']\n",
    "    }\n",
    "\n",
    "    for cat, keywords in categories.items():\n",
    "        features[f'cat_{cat}'] = int(any(kw in text_lower for kw in keywords))\n",
    "\n",
    "    # Premium/quality signals\n",
    "    features['premium'] = int(any(w in text_lower for w in ['premium', 'luxury', 'gourmet', 'deluxe']))\n",
    "    features['organic'] = int(any(w in text_lower for w in ['organic', 'natural', 'non-gmo']))\n",
    "    features['bulk'] = int(any(w in text_lower for w in ['bulk', 'wholesale', 'case']))\n",
    "\n",
    "    # All numbers in text\n",
    "    numbers = [float(x) for x in re.findall(r'\\d+\\.?\\d*', text_str)]\n",
    "    if numbers:\n",
    "        features['num_count'] = len(numbers)\n",
    "        features['num_max'] = max(numbers)\n",
    "        features['num_mean'] = np.mean(numbers)\n",
    "        features['num_std'] = np.std(numbers) if len(numbers) > 1 else 0\n",
    "    else:\n",
    "        features['num_count'] = 0\n",
    "        features['num_max'] = features['num_mean'] = features['num_std'] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "print(\"Parsing train catalogs...\")\n",
    "cat_train = pd.DataFrame([parse_catalog_comprehensive(t) for t in train_df['catalog_content']])\n",
    "print(\"Parsing test catalogs...\")\n",
    "cat_test = pd.DataFrame([parse_catalog_comprehensive(t) for t in test_df['catalog_content']])\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TARGET ENCODING FOR BRANDS\n",
    "# =============================================================================\n",
    "print(\"\\n[3/12] Target encoding brands...\")\n",
    "\n",
    "def target_encode_kfold(train_series, target, test_series, n_folds=5):\n",
    "    \"\"\"KFold target encoding to prevent leakage\"\"\"\n",
    "    global_mean = target.mean()\n",
    "\n",
    "    # Train encoding with KFold\n",
    "    train_encoded = np.zeros(len(train_series))\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(train_series)):\n",
    "        # Calculate means on train fold\n",
    "        stats = pd.DataFrame({\n",
    "            'cat': train_series.iloc[tr_idx],\n",
    "            'target': target.iloc[tr_idx]\n",
    "        }).groupby('cat')['target'].mean()\n",
    "\n",
    "        # Encode validation fold\n",
    "        train_encoded[val_idx] = train_series.iloc[val_idx].map(stats).fillna(global_mean).values\n",
    "\n",
    "    # Test encoding (use full train)\n",
    "    full_stats = pd.DataFrame({\n",
    "        'cat': train_series,\n",
    "        'target': target\n",
    "    }).groupby('cat')['target'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "    test_encoded = test_series.map(full_stats['mean']).fillna(global_mean)\n",
    "    test_std = test_series.map(full_stats['std']).fillna(0)\n",
    "    test_count = test_series.map(full_stats['count']).fillna(0)\n",
    "\n",
    "    train_std = train_series.map(full_stats['std']).fillna(0)\n",
    "    train_count = train_series.map(full_stats['count']).fillna(0)\n",
    "\n",
    "    return train_encoded, train_std, train_count, test_encoded, test_std, test_count\n",
    "\n",
    "brand_enc_tr, brand_std_tr, brand_cnt_tr, brand_enc_te, brand_std_te, brand_cnt_te = \\\n",
    "    target_encode_kfold(cat_train['brand'], train_df['price'], cat_test['brand'])\n",
    "\n",
    "cat_train['brand_target_mean'] = brand_enc_tr\n",
    "cat_train['brand_target_std'] = brand_std_tr\n",
    "cat_train['brand_count'] = brand_cnt_tr\n",
    "\n",
    "cat_test['brand_target_mean'] = brand_enc_te\n",
    "cat_test['brand_target_std'] = brand_std_te\n",
    "cat_test['brand_count'] = brand_cnt_te\n",
    "\n",
    "# Drop string column\n",
    "cat_train = cat_train.drop(columns=['brand'])\n",
    "cat_test = cat_test.drop(columns=['brand'])\n",
    "\n",
    "# Encode unit_type to numeric\n",
    "if 'unit_type' in cat_train.columns:\n",
    "    unit_mapping = {unit: i for i, unit in enumerate(cat_train['unit_type'].unique())}\n",
    "    cat_train['unit_type'] = cat_train['unit_type'].map(unit_mapping).fillna(0)\n",
    "    cat_test['unit_type'] = cat_test['unit_type'].map(unit_mapping).fillna(0)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. EMBEDDING STATISTICAL FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[4/12] Creating embedding statistical features...\")\n",
    "\n",
    "def create_emb_stats(emb):\n",
    "    \"\"\"Create statistical features from embeddings\"\"\"\n",
    "    stats = pd.DataFrame({\n",
    "        'mean': emb.mean(axis=1),\n",
    "        'std': emb.std(axis=1),\n",
    "        'min': emb.min(axis=1),\n",
    "        'max': emb.max(axis=1),\n",
    "        'median': np.median(emb, axis=1),\n",
    "        'q25': np.percentile(emb, 25, axis=1),\n",
    "        'q75': np.percentile(emb, 75, axis=1),\n",
    "        'norm': np.linalg.norm(emb, axis=1),\n",
    "        'skew': [skew(row) for row in emb],\n",
    "        'kurt': [kurtosis(row) for row in emb]\n",
    "    })\n",
    "    return stats\n",
    "\n",
    "text_stats_tr = create_emb_stats(X_text_train)\n",
    "text_stats_tr.columns = ['text_' + c for c in text_stats_tr.columns]\n",
    "\n",
    "text_stats_te = create_emb_stats(X_text_test)\n",
    "text_stats_te.columns = ['text_' + c for c in text_stats_te.columns]\n",
    "\n",
    "img_stats_tr = create_emb_stats(X_img_train)\n",
    "img_stats_tr.columns = ['img_' + c for c in img_stats_tr.columns]\n",
    "\n",
    "img_stats_te = create_emb_stats(X_img_test)\n",
    "img_stats_te.columns = ['img_' + c for c in img_stats_te.columns]\n",
    "\n",
    "# =============================================================================\n",
    "# 5. CROSS-MODAL FEATURES (CRITICAL!)\n",
    "# =============================================================================\n",
    "print(\"\\n[5/12] Computing cross-modal text-image features...\")\n",
    "\n",
    "def compute_cross_modal(text_emb, img_emb):\n",
    "    \"\"\"Text-image interaction features - these are GOLD for pricing!\"\"\"\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    text_norm = text_emb / (np.linalg.norm(text_emb, axis=1, keepdims=True) + 1e-8)\n",
    "    img_norm = img_emb / (np.linalg.norm(img_emb, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    features = pd.DataFrame({\n",
    "        # Cosine similarity\n",
    "        'cosine_sim': (text_norm * img_norm).sum(axis=1),\n",
    "\n",
    "        # Euclidean distance\n",
    "        'euclidean_dist': np.linalg.norm(text_emb - img_emb, axis=1),\n",
    "\n",
    "        # Manhattan distance\n",
    "        'manhattan_dist': np.abs(text_emb - img_emb).sum(axis=1),\n",
    "\n",
    "        # Element-wise product stats\n",
    "        'product_mean': (text_emb * img_emb).mean(axis=1),\n",
    "        'product_std': (text_emb * img_emb).std(axis=1),\n",
    "        'product_max': (text_emb * img_emb).max(axis=1),\n",
    "        'product_min': (text_emb * img_emb).min(axis=1),\n",
    "\n",
    "        # Difference stats\n",
    "        'diff_mean': np.abs(text_emb - img_emb).mean(axis=1),\n",
    "        'diff_std': np.abs(text_emb - img_emb).std(axis=1),\n",
    "        'diff_max': np.abs(text_emb - img_emb).max(axis=1),\n",
    "\n",
    "        # Norm ratios\n",
    "        'norm_ratio': np.linalg.norm(text_emb, axis=1) / (np.linalg.norm(img_emb, axis=1) + 1e-8),\n",
    "\n",
    "        # Correlation (row-wise)\n",
    "        'correlation': [np.corrcoef(t, i)[0, 1] if not np.any(np.isnan([t, i])) else 0\n",
    "                       for t, i in zip(text_emb, img_emb)]\n",
    "    })\n",
    "\n",
    "    return features.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "cross_modal_tr = compute_cross_modal(X_text_train, X_img_train)\n",
    "cross_modal_te = compute_cross_modal(X_text_test, X_img_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. PCA ON EMBEDDINGS (KEEP MORE DIMS!)\n",
    "# =============================================================================\n",
    "print(\"\\n[6/12] PCA on embeddings (keeping 256 dims)...\")\n",
    "\n",
    "# Text PCA - keep 256 dims (much better than 64!)\n",
    "pca_text = PCA(n_components=256, random_state=SEED)\n",
    "text_pca_tr = pca_text.fit_transform(X_text_train)\n",
    "text_pca_te = pca_text.transform(X_text_test)\n",
    "print(f\"Text PCA explained variance: {pca_text.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Image PCA\n",
    "pca_img = PCA(n_components=128, random_state=SEED)\n",
    "img_pca_tr = pca_img.fit_transform(X_img_train)\n",
    "img_pca_te = pca_img.transform(X_img_test)\n",
    "print(f\"Image PCA explained variance: {pca_img.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "text_pca_df_tr = pd.DataFrame(text_pca_tr, columns=[f'text_pca{i}' for i in range(256)])\n",
    "text_pca_df_te = pd.DataFrame(text_pca_te, columns=[f'text_pca{i}' for i in range(256)])\n",
    "\n",
    "img_pca_df_tr = pd.DataFrame(img_pca_tr, columns=[f'img_pca{i}' for i in range(128)])\n",
    "img_pca_df_te = pd.DataFrame(img_pca_te, columns=[f'img_pca{i}' for i in range(128)])\n",
    "\n",
    "# =============================================================================\n",
    "# 7. KNN PRICE FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[7/12] Computing KNN neighborhood price features...\")\n",
    "\n",
    "# Use combined embedding space\n",
    "combined_tr = np.hstack([text_pca_tr[:, :64], img_pca_tr[:, :64]])  # 128 dims for KNN\n",
    "combined_te = np.hstack([text_pca_te[:, :64], img_pca_te[:, :64]])\n",
    "\n",
    "K = 20\n",
    "nbrs = NearestNeighbors(n_neighbors=K+1, n_jobs=-1)\n",
    "nbrs.fit(combined_tr)\n",
    "\n",
    "# Train KNN\n",
    "dist_tr, idx_tr = nbrs.kneighbors(combined_tr)\n",
    "knn_prices_tr = np.array([train_df['price'].iloc[idx_tr[i, 1:]].values for i in range(len(train_df))])\n",
    "\n",
    "knn_tr = pd.DataFrame({\n",
    "    'knn_price_mean': knn_prices_tr.mean(axis=1),\n",
    "    'knn_price_median': np.median(knn_prices_tr, axis=1),\n",
    "    'knn_price_std': knn_prices_tr.std(axis=1),\n",
    "    'knn_price_min': knn_prices_tr.min(axis=1),\n",
    "    'knn_price_max': knn_prices_tr.max(axis=1),\n",
    "    'knn_price_range': knn_prices_tr.max(axis=1) - knn_prices_tr.min(axis=1),\n",
    "    'knn_dist_mean': dist_tr[:, 1:].mean(axis=1),\n",
    "    'knn_dist_min': dist_tr[:, 1:].min(axis=1)\n",
    "})\n",
    "\n",
    "# Test KNN\n",
    "dist_te, idx_te = nbrs.kneighbors(combined_te)\n",
    "knn_prices_te = np.array([train_df['price'].iloc[idx_te[i, :]].values for i in range(len(test_df))])\n",
    "\n",
    "knn_te = pd.DataFrame({\n",
    "    'knn_price_mean': knn_prices_te.mean(axis=1),\n",
    "    'knn_price_median': np.median(knn_prices_te, axis=1),\n",
    "    'knn_price_std': knn_prices_te.std(axis=1),\n",
    "    'knn_price_min': knn_prices_te.min(axis=1),\n",
    "    'knn_price_max': knn_prices_te.max(axis=1),\n",
    "    'knn_price_range': knn_prices_te.max(axis=1) - knn_prices_te.min(axis=1),\n",
    "    'knn_dist_mean': dist_te.mean(axis=1),\n",
    "    'knn_dist_min': dist_te.min(axis=1)\n",
    "})\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TFIDF FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[8/12] TF-IDF features...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=3)\n",
    "tfidf_tr = tfidf.fit_transform(train_df['catalog_content'].fillna(''))\n",
    "tfidf_te = tfidf.transform(test_df['catalog_content'].fillna(''))\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=SEED)\n",
    "tfidf_svd_tr = svd.fit_transform(tfidf_tr)\n",
    "tfidf_svd_te = svd.transform(tfidf_te)\n",
    "\n",
    "tfidf_df_tr = pd.DataFrame(tfidf_svd_tr, columns=[f'tfidf{i}' for i in range(50)])\n",
    "tfidf_df_te = pd.DataFrame(tfidf_svd_te, columns=[f'tfidf{i}' for i in range(50)])\n",
    "\n",
    "# =============================================================================\n",
    "# 9. COMBINE ALL FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[9/12] Combining all features...\")\n",
    "\n",
    "X_train = pd.concat([\n",
    "    cat_train.reset_index(drop=True),\n",
    "    text_stats_tr.reset_index(drop=True),\n",
    "    img_stats_tr.reset_index(drop=True),\n",
    "    cross_modal_tr.reset_index(drop=True),\n",
    "    text_pca_df_tr.reset_index(drop=True),\n",
    "    img_pca_df_tr.reset_index(drop=True),\n",
    "    knn_tr.reset_index(drop=True),\n",
    "    tfidf_df_tr.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    cat_test.reset_index(drop=True),\n",
    "    text_stats_te.reset_index(drop=True),\n",
    "    img_stats_te.reset_index(drop=True),\n",
    "    cross_modal_te.reset_index(drop=True),\n",
    "    text_pca_df_te.reset_index(drop=True),\n",
    "    img_pca_df_te.reset_index(drop=True),\n",
    "    knn_te.reset_index(drop=True),\n",
    "    tfidf_df_te.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Feature matrix: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# Clean data\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Align columns\n",
    "for col in X_train.columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "y = train_df['price'].values\n",
    "\n",
    "# Light outlier removal (keep 99% of data)\n",
    "q01, q99 = np.percentile(y, [0.5, 99.5])\n",
    "mask = (y >= q01) & (y <= q99)\n",
    "X_train = X_train[mask].reset_index(drop=True)\n",
    "y = y[mask]\n",
    "print(f\"Removed {sum(~mask)} outliers, keeping {len(y)} samples\")\n",
    "\n",
    "# =============================================================================\n",
    "# 10. SCALE FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[10/12] Scaling features...\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 11. TRAIN MODELS - FIXED VERSION\n",
    "# =============================================================================\n",
    "print(\"\\n[11/12] Training ensemble models...\")\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denom = np.abs(y_true) + np.abs(y_pred)\n",
    "    denom[denom == 0] = 1\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_preds = {}\n",
    "test_preds = {}\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "oof_preds['xgb'] = np.zeros(len(X_train_scaled))\n",
    "test_preds['xgb'] = np.zeros(len(X_test_scaled))\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_scaled), 1):\n",
    "    Xtr, Xval = X_train_scaled[tr_idx], X_train_scaled[val_idx]\n",
    "    ytr, yval = y_log[tr_idx], y_log[val_idx]\n",
    "\n",
    "    # --- THIS IS THE FIX ---\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=1200,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.04,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=2.0,\n",
    "        min_child_weight=3,\n",
    "        tree_method='gpu_hist' if USE_GPU else 'hist',\n",
    "        random_state=SEED,\n",
    "        verbosity=0,\n",
    "        early_stopping_rounds=100 # <-- The parameter is now part of the definition\n",
    "    )\n",
    "    # The .fit() call is now cleaner and correct\n",
    "    xgb.fit(Xtr, ytr, eval_set=[(Xval, yval)], verbose=False)\n",
    "\n",
    "    oof_preds['xgb'][val_idx] = xgb.predict(Xval)\n",
    "    test_preds['xgb'] += xgb.predict(X_test_scaled) / N_FOLDS\n",
    "\n",
    "    val_smape = smape(np.expm1(yval), np.expm1(xgb.predict(Xval)))\n",
    "    print(f\"  Fold {fold} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# LightGBM with LOG TRANSFORM (FIXED!)\n",
    "# =============================================================================\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "oof_preds['lgb'] = np.zeros(len(X_train_scaled))\n",
    "test_preds['lgb'] = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_scaled), 1):\n",
    "    Xtr, Xval = X_train_scaled[tr_idx], X_train_scaled[val_idx]\n",
    "    ytr, yval = y_log[tr_idx], y_log[val_idx]  # Use LOG transform!\n",
    "\n",
    "    lgb = LGBMRegressor(\n",
    "        n_estimators=1500,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.04,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=2.0,\n",
    "        min_child_samples=20,\n",
    "        objective='regression',  # Standard regression, NOT MAPE!\n",
    "        metric='l2',\n",
    "        device='gpu' if USE_GPU else 'cpu',\n",
    "        random_state=SEED,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    lgb.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=[(Xval, yval)],\n",
    "        callbacks=[early_stopping(stopping_rounds=100),\n",
    "                  log_evaluation(period=0)]\n",
    "    )\n",
    "\n",
    "    # Predict in log space, then convert back\n",
    "    val_pred = np.expm1(lgb.predict(Xval))\n",
    "    oof_preds['lgb'][val_idx] = val_pred\n",
    "    test_preds['lgb'] += np.expm1(lgb.predict(X_test_scaled)) / N_FOLDS\n",
    "\n",
    "    val_smape = smape(np.expm1(yval), val_pred)\n",
    "    print(f\"  Fold {fold} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# CatBoost with LOG TRANSFORM (FIXED!)\n",
    "# =============================================================================\n",
    "print(\"\\nTraining CatBoost...\")\n",
    "oof_preds['cat'] = np.zeros(len(X_train_scaled))\n",
    "test_preds['cat'] = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_scaled), 1):\n",
    "    Xtr, Xval = X_train_scaled[tr_idx], X_train_scaled[val_idx]\n",
    "    ytr, yval = y_log[tr_idx], y_log[val_idx]  # Use LOG transform!\n",
    "\n",
    "    cat = CatBoostRegressor(\n",
    "        iterations=1500,\n",
    "        depth=6,\n",
    "        learning_rate=0.04,\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function='RMSE',  # Standard RMSE, NOT MAPE!\n",
    "        eval_metric='RMSE',\n",
    "        task_type='GPU' if USE_GPU else 'CPU',\n",
    "        random_state=SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    cat.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=(Xval, yval),\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    # Predict in log space, then convert back\n",
    "    val_pred = np.expm1(cat.predict(Xval))\n",
    "    oof_preds['cat'][val_idx] = val_pred\n",
    "    test_preds['cat'] += np.expm1(cat.predict(X_test_scaled)) / N_FOLDS\n",
    "\n",
    "    val_smape = smape(np.expm1(yval), val_pred)\n",
    "    print(f\"  Fold {fold} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 12. META ENSEMBLE\n",
    "# =============================================================================\n",
    "print(\"\\n[12/12] Meta ensemble with SMAPE-optimized blending...\")\n",
    "\n",
    "# Convert XGB predictions back from log space\n",
    "oof_preds['xgb'] = np.expm1(oof_preds['xgb'])\n",
    "test_preds['xgb'] = np.expm1(test_preds['xgb'])\n",
    "\n",
    "# Check individual model scores\n",
    "print(\"\\nIndividual model OOF SMAPE:\")\n",
    "for name in ['xgb', 'lgb', 'cat']:\n",
    "    score = smape(y, oof_preds[name])\n",
    "    print(f\"  {name.upper()}: {score:.2f}%\")\n",
    "\n",
    "# Custom SMAPE-optimized blending\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def smape_loss(weights):\n",
    "    weights = np.abs(weights) / (np.abs(weights).sum() + 1e-10)\n",
    "    blend = sum(w * oof_preds[name] for w, name in zip(weights, ['xgb', 'lgb', 'cat']))\n",
    "    return smape(y, blend)\n",
    "\n",
    "# Find optimal weights\n",
    "result = minimize(smape_loss, x0=[0.33, 0.33, 0.34], method='Nelder-Mead',\n",
    "                 options={'maxiter': 1000})\n",
    "optimal_weights = np.abs(result.x) / (np.abs(result.x).sum() + 1e-10)\n",
    "\n",
    "print(f\"\\nOptimal blend weights: XGB={optimal_weights[0]:.3f}, \"\n",
    "      f\"LGB={optimal_weights[1]:.3f}, CAT={optimal_weights[2]:.3f}\")\n",
    "\n",
    "# Final predictions\n",
    "final_oof = sum(w * oof_preds[name] for w, name in zip(optimal_weights, ['xgb', 'lgb', 'cat']))\n",
    "final_test = sum(w * test_preds[name] for w, name in zip(optimal_weights, ['xgb', 'lgb', 'cat']))\n",
    "\n",
    "# Post-processing: clip extremes\n",
    "final_test = np.clip(final_test, y.min() * 0.8, y.max() * 1.1)\n",
    "\n",
    "final_smape = smape(y, final_oof)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL OOF SMAPE: {final_smape:.2f}%\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE PREDICTIONS\n",
    "# =============================================================================\n",
    "print(f\"\\nSaving predictions to: {OUTPUT_CSV}\")\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': final_test\n",
    "})\n",
    "submission = submission.sort_values('sample_id').reset_index(drop=True)\n",
    "submission.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n✅ DONE!\")\n",
    "print(f\"Predicted price range: ${submission['price'].min():.2f} - ${submission['price'].max():.2f}\")\n",
    "print(f\"Mean predicted price: ${submission['price'].mean():.2f}\")\n",
    "print(f\"Median predicted price: ${submission['price'].median():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
